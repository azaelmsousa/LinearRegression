{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.pkl', 'diamonds.csv', 'train.pkl']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from pandas.api.types import CategoricalDtype\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "from sklearn import model_selection, ensemble, metrics, linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import *\n",
    "import os\n",
    "base_dir = '../data'\n",
    "print(os.listdir(base_dir))\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output.https://www.kaggle.com/wanermiranda/linear-regression-ml-tp1?scriptVersionId=5240484"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.24</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>J</td>\n",
       "      <td>VVS2</td>\n",
       "      <td>62.8</td>\n",
       "      <td>57.0</td>\n",
       "      <td>336</td>\n",
       "      <td>3.94</td>\n",
       "      <td>3.96</td>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.24</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>I</td>\n",
       "      <td>VVS1</td>\n",
       "      <td>62.3</td>\n",
       "      <td>57.0</td>\n",
       "      <td>336</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.26</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>61.9</td>\n",
       "      <td>55.0</td>\n",
       "      <td>337</td>\n",
       "      <td>4.07</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.22</td>\n",
       "      <td>Fair</td>\n",
       "      <td>E</td>\n",
       "      <td>VS2</td>\n",
       "      <td>65.1</td>\n",
       "      <td>61.0</td>\n",
       "      <td>337</td>\n",
       "      <td>3.87</td>\n",
       "      <td>3.78</td>\n",
       "      <td>2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>H</td>\n",
       "      <td>VS1</td>\n",
       "      <td>59.4</td>\n",
       "      <td>61.0</td>\n",
       "      <td>338</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.05</td>\n",
       "      <td>2.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    carat        cut color clarity  depth  table  price     x     y     z\n",
       "1    0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "2    0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "3    0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "4    0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "5    0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n",
       "6    0.24  Very Good     J    VVS2   62.8   57.0    336  3.94  3.96  2.48\n",
       "7    0.24  Very Good     I    VVS1   62.3   57.0    336  3.95  3.98  2.47\n",
       "8    0.26  Very Good     H     SI1   61.9   55.0    337  4.07  4.11  2.53\n",
       "9    0.22       Fair     E     VS2   65.1   61.0    337  3.87  3.78  2.49\n",
       "10   0.23  Very Good     H     VS1   59.4   61.0    338  4.00  4.05  2.39"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_diamonds = pd.read_csv('%s/diamonds.csv'%(base_dir), index_col='Unnamed: 0')\n",
    "df_diamonds.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "877abad593a3e39cd4e76d7dbad18288dae62e08"
   },
   "source": [
    "## Numeric Features \n",
    "* Carat: weight of the diamond\n",
    "* depth: depth %  The height of a diamond, measured from the culet to the table, divided by its average girdle diameter\n",
    "* table: table % The width of the diamond's table expressed as a percentage of its average diameter\n",
    "* price: the price of the diamond\n",
    "* xlength: mm\n",
    "* ywidth: mm\n",
    "* zdepth: mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "98385ec227afd7a3f5ff08073ffc07795d54785e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.797940</td>\n",
       "      <td>61.749405</td>\n",
       "      <td>57.457184</td>\n",
       "      <td>3932.799722</td>\n",
       "      <td>5.731157</td>\n",
       "      <td>5.734526</td>\n",
       "      <td>3.538734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.474011</td>\n",
       "      <td>1.432621</td>\n",
       "      <td>2.234491</td>\n",
       "      <td>3989.439738</td>\n",
       "      <td>1.121761</td>\n",
       "      <td>1.142135</td>\n",
       "      <td>0.705699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>2401.000000</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>5324.250000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.010000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>18823.000000</td>\n",
       "      <td>10.740000</td>\n",
       "      <td>58.900000</td>\n",
       "      <td>31.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              carat         depth         table         price             x  \\\n",
       "count  53940.000000  53940.000000  53940.000000  53940.000000  53940.000000   \n",
       "mean       0.797940     61.749405     57.457184   3932.799722      5.731157   \n",
       "std        0.474011      1.432621      2.234491   3989.439738      1.121761   \n",
       "min        0.200000     43.000000     43.000000    326.000000      0.000000   \n",
       "25%        0.400000     61.000000     56.000000    950.000000      4.710000   \n",
       "50%        0.700000     61.800000     57.000000   2401.000000      5.700000   \n",
       "75%        1.040000     62.500000     59.000000   5324.250000      6.540000   \n",
       "max        5.010000     79.000000     95.000000  18823.000000     10.740000   \n",
       "\n",
       "                  y             z  \n",
       "count  53940.000000  53940.000000  \n",
       "mean       5.734526      3.538734  \n",
       "std        1.142135      0.705699  \n",
       "min        0.000000      0.000000  \n",
       "25%        4.720000      2.910000  \n",
       "50%        5.710000      3.530000  \n",
       "75%        6.540000      4.040000  \n",
       "max       58.900000     31.800000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_diamonds.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ee6d1d43b7eff5db4be04ee0c292367c50d160d"
   },
   "source": [
    "## cut \n",
    "Describe cut quality of the diamond. Quality in increasing order Fair, Good, Very Good, Premium, Ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "787f5714a8636051ff3874ec45c7f7c10c060b75",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ideal, Premium, Good, Very Good, Fair]\n",
      "Categories (5, object): [Fair < Good < Very Good < Premium < Ideal]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     53940\n",
       "unique        5\n",
       "top       Ideal\n",
       "freq      21551\n",
       "Name: cut, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuts_ordered = ['Fair',\n",
    "                'Good',\n",
    "                'Very Good',\n",
    "                'Premium',\n",
    "                'Ideal']\n",
    "df_diamonds['cut'] = df_diamonds['cut'].astype(CategoricalDtype(cuts_ordered, ordered=True))\n",
    "print(df_diamonds['cut'].unique())\n",
    "df_diamonds['cut'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4210c64cd66c7200e888b8198781070dc99c6542"
   },
   "source": [
    "## color\n",
    "mColor of the diamond, with D being the best and J the worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "027047bb1b67635c3487c2d01a92603dc8abfc3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E, I, J, H, F, G, D]\n",
      "Categories (7, object): [J < I < H < G < F < E < D]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     53940\n",
       "unique        7\n",
       "top           G\n",
       "freq      11292\n",
       "Name: color, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors_ordered = [  'J',\n",
    "                    'I',\n",
    "                    'H',\n",
    "                    'G',\n",
    "                    'F',\n",
    "                    'E',\n",
    "                    'D']\n",
    "df_diamonds['color'] = df_diamonds['color'].astype(CategoricalDtype(colors_ordered, ordered=True))\n",
    "print(df_diamonds['color'].unique())\n",
    "df_diamonds['color'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f3a28f822e662d4a4bfd60408d2547e62e670047"
   },
   "source": [
    "## clarity\n",
    "How obvious inclusions are within the diamond:(in order from best to worst, FL = flawless, I3= level 3 inclusions) FL,IF, VVS1, VVS2, VS1, VS2, SI1, SI2, I1, I2, I3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "3b1bfb5329e161298d386c5310c1a78cb46974fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SI2, SI1, VS1, VS2, VVS2, VVS1, I1, IF]\n",
      "Categories (8, object): [SI2 < SI1 < VS2 < IF < VVS1 < VS1 < I1 < VVS2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     53940\n",
       "unique        8\n",
       "top         SI1\n",
       "freq      13065\n",
       "Name: clarity, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clarity_codes = {'I3',\n",
    "'I2',\n",
    "'I1',\n",
    "'SI2',\n",
    "'SI1',\n",
    "'VS2',\n",
    "'VS1',\n",
    "'VVS2',\n",
    "'VVS1',\n",
    "'IF',\n",
    "'FL'}\n",
    "df_diamonds['clarity'] = df_diamonds['clarity'].astype(CategoricalDtype(clarity_codes, ordered=True))\n",
    "print(df_diamonds['clarity'].unique())\n",
    "df_diamonds['clarity'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff600d5845d90675104880a893d97727c81e990e"
   },
   "source": [
    "## Cleaning the Data\n",
    "There are some zero dimensions for the diamonds, since that must be noise or mistype, we are cleaning it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "12857a643f144df6f855a07a82e6c80a4d7720bd"
   },
   "outputs": [],
   "source": [
    "df_diamonds = df_diamonds.drop(df_diamonds.loc[df_diamonds.x <= 0].index)\n",
    "df_diamonds = df_diamonds.drop(df_diamonds.loc[df_diamonds.y <= 0].index)\n",
    "df_diamonds = df_diamonds.drop(df_diamonds.loc[df_diamonds.z <= 0].index)\n",
    "df_diamonds = df_diamonds.drop(df_diamonds.loc[df_diamonds.carat <= 0].index)\n",
    "df_diamonds = df_diamonds.drop(df_diamonds.loc[df_diamonds.depth <= 0].index)\n",
    "df_diamonds = df_diamonds.drop(df_diamonds.loc[df_diamonds.table <= 0].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "23d38491d85c45459fe2b582f1b460c9dfa76891"
   },
   "source": [
    "## Handcraft features\n",
    "Since the measures for the diamond follow a 3d shape, we are considering here some handcraft features. \n",
    "Volume for the diamond = reflecting its size and weight. \n",
    "Ratio between the X, Y and Z.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "f14e6c21aa6070199e551fcaab1bd3e248591ad7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>volume</th>\n",
       "      <th>ratio_xy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "      <td>38.202030</td>\n",
       "      <td>0.992462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "      <td>34.505856</td>\n",
       "      <td>1.013021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "      <td>38.076885</td>\n",
       "      <td>0.995086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "      <td>46.724580</td>\n",
       "      <td>0.992908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "      <td>51.917250</td>\n",
       "      <td>0.997701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.24</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>J</td>\n",
       "      <td>VVS2</td>\n",
       "      <td>62.8</td>\n",
       "      <td>57.0</td>\n",
       "      <td>336</td>\n",
       "      <td>3.94</td>\n",
       "      <td>3.96</td>\n",
       "      <td>2.48</td>\n",
       "      <td>38.693952</td>\n",
       "      <td>0.994949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.24</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>I</td>\n",
       "      <td>VVS1</td>\n",
       "      <td>62.3</td>\n",
       "      <td>57.0</td>\n",
       "      <td>336</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.47</td>\n",
       "      <td>38.830870</td>\n",
       "      <td>0.992462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.26</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>61.9</td>\n",
       "      <td>55.0</td>\n",
       "      <td>337</td>\n",
       "      <td>4.07</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.53</td>\n",
       "      <td>42.321081</td>\n",
       "      <td>0.990268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.22</td>\n",
       "      <td>Fair</td>\n",
       "      <td>E</td>\n",
       "      <td>VS2</td>\n",
       "      <td>65.1</td>\n",
       "      <td>61.0</td>\n",
       "      <td>337</td>\n",
       "      <td>3.87</td>\n",
       "      <td>3.78</td>\n",
       "      <td>2.49</td>\n",
       "      <td>36.425214</td>\n",
       "      <td>1.023810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>H</td>\n",
       "      <td>VS1</td>\n",
       "      <td>59.4</td>\n",
       "      <td>61.0</td>\n",
       "      <td>338</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.05</td>\n",
       "      <td>2.39</td>\n",
       "      <td>38.718000</td>\n",
       "      <td>0.987654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    carat        cut color clarity  depth  table  price     x     y     z  \\\n",
       "1    0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43   \n",
       "2    0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31   \n",
       "3    0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31   \n",
       "4    0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63   \n",
       "5    0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75   \n",
       "6    0.24  Very Good     J    VVS2   62.8   57.0    336  3.94  3.96  2.48   \n",
       "7    0.24  Very Good     I    VVS1   62.3   57.0    336  3.95  3.98  2.47   \n",
       "8    0.26  Very Good     H     SI1   61.9   55.0    337  4.07  4.11  2.53   \n",
       "9    0.22       Fair     E     VS2   65.1   61.0    337  3.87  3.78  2.49   \n",
       "10   0.23  Very Good     H     VS1   59.4   61.0    338  4.00  4.05  2.39   \n",
       "\n",
       "       volume  ratio_xy  \n",
       "1   38.202030  0.992462  \n",
       "2   34.505856  1.013021  \n",
       "3   38.076885  0.995086  \n",
       "4   46.724580  0.992908  \n",
       "5   51.917250  0.997701  \n",
       "6   38.693952  0.994949  \n",
       "7   38.830870  0.992462  \n",
       "8   42.321081  0.990268  \n",
       "9   36.425214  1.023810  \n",
       "10  38.718000  0.987654  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_diamonds['volume'] = df_diamonds['x'] * df_diamonds['y'] * df_diamonds['z']\n",
    "df_diamonds['ratio_xy'] = df_diamonds['x'] / df_diamonds['y']\n",
    "# df_diamonds['ratio_xz'] = df_diamonds['x'] / df_diamonds['z']\n",
    "df_diamonds.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "d9fd0904b92ed37e3febcd2fd79e1e56339de40b"
   },
   "outputs": [],
   "source": [
    "train, test_reserved = model_selection.train_test_split(df_diamonds, test_size=0.2, random_state=42)\n",
    "test_reserved.to_csv('test.csv')\n",
    "train.to_csv('train.csv')\n",
    "\n",
    "df_diamonds = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40aa323183c465324b80d853e8078f98861d1cb6"
   },
   "source": [
    "# Distribution Overview\n",
    "The prices seems to follow a power law curve, as show bellow in the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "631192439fb586987bb4b0dbe6ab86cab9688264"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f852f8c0828>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_diamonds['price'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "cd3f5d223b0fd5d22c1f20efac39206928b9356e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    43136.000000\n",
       "mean      3926.531459\n",
       "std       3983.079043\n",
       "min        326.000000\n",
       "25%        950.750000\n",
       "50%       2398.000000\n",
       "75%       5330.000000\n",
       "max      18806.000000\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_diamonds['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "633be5cf172d1162ebb0c25799d24dc9a93a21c2"
   },
   "source": [
    "## SGD Regression For Fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "64f46dea7e0ad812b0a31fb32dad8e1b1033f4f4"
   },
   "outputs": [],
   "source": [
    "cat_columns = df_diamonds.select_dtypes(['category']).columns.values\n",
    "df_diamonds[cat_columns] = df_diamonds[cat_columns].apply(lambda x: x.cat.codes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a5882811ccd8819456a7eba412305d5c03b80e9"
   },
   "source": [
    "## Normalizing the Data\n",
    "Using the robust scaller **to not only use the mean normalization**, but also to be less vulnerable to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "4e961645520c3d8c27f8975ba6e78a0dd2ca1d1a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X  = df_diamonds.copy()\n",
    "y = X.pop('price')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ac3b7f88def624b3ff237cfbe2321091c8559886"
   },
   "source": [
    "## Regression\n",
    "Since there is no negative values in the prices we are using here the log(price) to maintain this domain during the regression train. \n",
    "We are also using a 5 cross fold validation to do the grid search. \n",
    "\n",
    "A validation set was extracted from the data as a simulation for the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "5ec10ea5e85d3356e8594bedb4e5292dbae376b7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import *\n",
    "\n",
    "X_train, X_val, y_train, y_val = model_selection.train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "ed88474e913c58b94b3b7b70c6a99dd9769b2b83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.98, NNZs: 11, Bias: 7.657853, T: 38822, Avg. loss: 0.182553\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.21, NNZs: 11, Bias: 7.724249, T: 77644, Avg. loss: 0.027089\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.39, NNZs: 11, Bias: 7.759975, T: 116466, Avg. loss: 0.022152\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.48, NNZs: 11, Bias: 7.770402, T: 155288, Avg. loss: 0.020564\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.54, NNZs: 11, Bias: 7.779908, T: 194110, Avg. loss: 0.019974\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.58, NNZs: 11, Bias: 7.795566, T: 232932, Avg. loss: 0.019719\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.60, NNZs: 11, Bias: 7.801189, T: 271754, Avg. loss: 0.019553\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.62, NNZs: 11, Bias: 7.804956, T: 310576, Avg. loss: 0.019487\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.64, NNZs: 11, Bias: 7.802267, T: 349398, Avg. loss: 0.019441\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.64, NNZs: 11, Bias: 7.803367, T: 388220, Avg. loss: 0.019448\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.65, NNZs: 11, Bias: 7.809271, T: 427042, Avg. loss: 0.019436\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.66, NNZs: 11, Bias: 7.810575, T: 465864, Avg. loss: 0.019426\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.66, NNZs: 11, Bias: 7.805513, T: 504686, Avg. loss: 0.019418\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.67, NNZs: 11, Bias: 7.806776, T: 543508, Avg. loss: 0.019416\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.67, NNZs: 11, Bias: 7.807903, T: 582330, Avg. loss: 0.019403\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.67, NNZs: 11, Bias: 7.805328, T: 621152, Avg. loss: 0.019399\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.67, NNZs: 11, Bias: 7.811337, T: 659974, Avg. loss: 0.019407\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.67, NNZs: 11, Bias: 7.806591, T: 698796, Avg. loss: 0.019403\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.67, NNZs: 11, Bias: 7.802215, T: 737618, Avg. loss: 0.019388\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1.67, NNZs: 11, Bias: 7.806693, T: 776440, Avg. loss: 0.019382\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1.67, NNZs: 11, Bias: 7.811555, T: 815262, Avg. loss: 0.019384\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1.67, NNZs: 11, Bias: 7.809826, T: 854084, Avg. loss: 0.019382\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1.67, NNZs: 11, Bias: 7.810954, T: 892906, Avg. loss: 0.019386\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1.68, NNZs: 11, Bias: 7.807940, T: 931728, Avg. loss: 0.019382\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1.67, NNZs: 11, Bias: 7.806240, T: 970550, Avg. loss: 0.019386\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1.68, NNZs: 11, Bias: 7.811602, T: 1009372, Avg. loss: 0.019376\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.68, NNZs: 11, Bias: 7.806442, T: 1048194, Avg. loss: 0.019378\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.68, NNZs: 11, Bias: 7.807570, T: 1087016, Avg. loss: 0.019366\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.68, NNZs: 11, Bias: 7.806984, T: 1125838, Avg. loss: 0.019368\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.68, NNZs: 11, Bias: 7.806499, T: 1164660, Avg. loss: 0.019361\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1.68, NNZs: 11, Bias: 7.807373, T: 1203482, Avg. loss: 0.019368\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1.68, NNZs: 11, Bias: 7.810640, T: 1242304, Avg. loss: 0.019370\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1.68, NNZs: 11, Bias: 7.805411, T: 1281126, Avg. loss: 0.019349\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1.68, NNZs: 11, Bias: 7.806581, T: 1319948, Avg. loss: 0.019368\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1.68, NNZs: 11, Bias: 7.809369, T: 1358770, Avg. loss: 0.019355\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1.68, NNZs: 11, Bias: 7.810050, T: 1397592, Avg. loss: 0.019362\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1.68, NNZs: 11, Bias: 7.809765, T: 1436414, Avg. loss: 0.019341\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1.68, NNZs: 11, Bias: 7.807043, T: 1475236, Avg. loss: 0.019353\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1.68, NNZs: 11, Bias: 7.809991, T: 1514058, Avg. loss: 0.019343\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1.68, NNZs: 11, Bias: 7.803261, T: 1552880, Avg. loss: 0.019337\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1.69, NNZs: 11, Bias: 7.807816, T: 1591702, Avg. loss: 0.019351\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1.69, NNZs: 11, Bias: 7.806099, T: 1630524, Avg. loss: 0.019352\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1.69, NNZs: 11, Bias: 7.808338, T: 1669346, Avg. loss: 0.019339\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1.69, NNZs: 11, Bias: 7.808981, T: 1708168, Avg. loss: 0.019342\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 1.69, NNZs: 11, Bias: 7.811389, T: 1746990, Avg. loss: 0.019327\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1.69, NNZs: 11, Bias: 7.806612, T: 1785812, Avg. loss: 0.019339\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 1.69, NNZs: 11, Bias: 7.804815, T: 1824634, Avg. loss: 0.019332\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 1.69, NNZs: 11, Bias: 7.807099, T: 1863456, Avg. loss: 0.019331\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1.69, NNZs: 11, Bias: 7.806462, T: 1902278, Avg. loss: 0.019333\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 1.69, NNZs: 11, Bias: 7.805514, T: 1941100, Avg. loss: 0.019324\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 1.70, NNZs: 11, Bias: 7.806717, T: 1979922, Avg. loss: 0.019329\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 1.69, NNZs: 11, Bias: 7.809003, T: 2018744, Avg. loss: 0.019327\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 1.70, NNZs: 11, Bias: 7.808372, T: 2057566, Avg. loss: 0.019325\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 1.70, NNZs: 11, Bias: 7.805631, T: 2096388, Avg. loss: 0.019322\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 1.70, NNZs: 11, Bias: 7.805313, T: 2135210, Avg. loss: 0.019323\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 1.70, NNZs: 11, Bias: 7.805485, T: 2174032, Avg. loss: 0.019320\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 1.70, NNZs: 11, Bias: 7.807185, T: 2212854, Avg. loss: 0.019325\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 1.70, NNZs: 11, Bias: 7.810748, T: 2251676, Avg. loss: 0.019317\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 1.70, NNZs: 11, Bias: 7.810281, T: 2290498, Avg. loss: 0.019314\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 1.70, NNZs: 11, Bias: 7.805519, T: 2329320, Avg. loss: 0.019316\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 1.70, NNZs: 11, Bias: 7.804490, T: 2368142, Avg. loss: 0.019315\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 1.70, NNZs: 11, Bias: 7.810948, T: 2406964, Avg. loss: 0.019304\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 1.70, NNZs: 11, Bias: 7.806017, T: 2445786, Avg. loss: 0.019304\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 1.70, NNZs: 11, Bias: 7.808322, T: 2484608, Avg. loss: 0.019313\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 1.71, NNZs: 11, Bias: 7.808477, T: 2523430, Avg. loss: 0.019306\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 1.71, NNZs: 11, Bias: 7.806923, T: 2562252, Avg. loss: 0.019304\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 1.70, NNZs: 11, Bias: 7.805663, T: 2601074, Avg. loss: 0.019302\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 1.71, NNZs: 11, Bias: 7.807381, T: 2639896, Avg. loss: 0.019301\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 1.71, NNZs: 11, Bias: 7.805153, T: 2678718, Avg. loss: 0.019302\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 1.71, NNZs: 11, Bias: 7.807948, T: 2717540, Avg. loss: 0.019299\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 1.71, NNZs: 11, Bias: 7.806229, T: 2756362, Avg. loss: 0.019297\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 1.71, NNZs: 11, Bias: 7.809831, T: 2795184, Avg. loss: 0.019297\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 1.71, NNZs: 11, Bias: 7.811051, T: 2834006, Avg. loss: 0.019293\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 1.71, NNZs: 11, Bias: 7.806559, T: 2872828, Avg. loss: 0.019287\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 1.71, NNZs: 11, Bias: 7.808178, T: 2911650, Avg. loss: 0.019296\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 1.71, NNZs: 11, Bias: 7.808923, T: 2950472, Avg. loss: 0.019289\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 1.72, NNZs: 11, Bias: 7.811732, T: 2989294, Avg. loss: 0.019292\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 1.71, NNZs: 11, Bias: 7.808781, T: 3028116, Avg. loss: 0.019295\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 1.71, NNZs: 11, Bias: 7.806690, T: 3066938, Avg. loss: 0.019289\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 1.72, NNZs: 11, Bias: 7.805061, T: 3105760, Avg. loss: 0.019289\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 1.72, NNZs: 11, Bias: 7.808128, T: 3144582, Avg. loss: 0.019289\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 1.72, NNZs: 11, Bias: 7.805847, T: 3183404, Avg. loss: 0.019291\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 1.72, NNZs: 11, Bias: 7.810822, T: 3222226, Avg. loss: 0.019278\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 1.72, NNZs: 11, Bias: 7.806530, T: 3261048, Avg. loss: 0.019283\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 1.72, NNZs: 11, Bias: 7.806713, T: 3299870, Avg. loss: 0.019286\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 1.72, NNZs: 11, Bias: 7.805692, T: 3338692, Avg. loss: 0.019285\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 1.72, NNZs: 11, Bias: 7.803601, T: 3377514, Avg. loss: 0.019283\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 1.72, NNZs: 11, Bias: 7.808111, T: 3416336, Avg. loss: 0.019280\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 1.73, NNZs: 11, Bias: 7.804490, T: 3455158, Avg. loss: 0.019276\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 1.73, NNZs: 11, Bias: 7.805408, T: 3493980, Avg. loss: 0.019281\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 1.73, NNZs: 11, Bias: 7.807089, T: 3532802, Avg. loss: 0.019279\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 1.73, NNZs: 11, Bias: 7.808041, T: 3571624, Avg. loss: 0.019269\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 1.73, NNZs: 11, Bias: 7.807147, T: 3610446, Avg. loss: 0.019271\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 1.73, NNZs: 11, Bias: 7.805387, T: 3649268, Avg. loss: 0.019277\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 1.73, NNZs: 11, Bias: 7.804141, T: 3688090, Avg. loss: 0.019267\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 1.73, NNZs: 11, Bias: 7.806611, T: 3726912, Avg. loss: 0.019276\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 1.73, NNZs: 11, Bias: 7.804137, T: 3765734, Avg. loss: 0.019274\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 1.73, NNZs: 11, Bias: 7.807910, T: 3804556, Avg. loss: 0.019266\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 1.73, NNZs: 11, Bias: 7.808324, T: 3843378, Avg. loss: 0.019264\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 1.73, NNZs: 11, Bias: 7.803161, T: 3882200, Avg. loss: 0.019269\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 1.74, NNZs: 11, Bias: 7.807648, T: 3921022, Avg. loss: 0.019271\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 1.73, NNZs: 11, Bias: 7.807014, T: 3959844, Avg. loss: 0.019269\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 1.74, NNZs: 11, Bias: 7.807837, T: 3998666, Avg. loss: 0.019261\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 1.74, NNZs: 11, Bias: 7.807313, T: 4037488, Avg. loss: 0.019265\n",
      "Total training time: 1.05 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 1.74, NNZs: 11, Bias: 7.805801, T: 4076310, Avg. loss: 0.019263\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 1.74, NNZs: 11, Bias: 7.802493, T: 4115132, Avg. loss: 0.019259\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 1.74, NNZs: 11, Bias: 7.804286, T: 4153954, Avg. loss: 0.019264\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 1.74, NNZs: 11, Bias: 7.808434, T: 4192776, Avg. loss: 0.019256\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 1.74, NNZs: 11, Bias: 7.807813, T: 4231598, Avg. loss: 0.019256\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 1.74, NNZs: 11, Bias: 7.802221, T: 4270420, Avg. loss: 0.019254\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 1.74, NNZs: 11, Bias: 7.807962, T: 4309242, Avg. loss: 0.019267\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 1.74, NNZs: 11, Bias: 7.807623, T: 4348064, Avg. loss: 0.019256\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 1.74, NNZs: 11, Bias: 7.809724, T: 4386886, Avg. loss: 0.019249\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 1.74, NNZs: 11, Bias: 7.807581, T: 4425708, Avg. loss: 0.019256\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 1.74, NNZs: 11, Bias: 7.805990, T: 4464530, Avg. loss: 0.019256\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 1.74, NNZs: 11, Bias: 7.805098, T: 4503352, Avg. loss: 0.019256\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 1.74, NNZs: 11, Bias: 7.805427, T: 4542174, Avg. loss: 0.019253\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 1.74, NNZs: 11, Bias: 7.806982, T: 4580996, Avg. loss: 0.019253\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 1.75, NNZs: 11, Bias: 7.805897, T: 4619818, Avg. loss: 0.019255\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 1.75, NNZs: 11, Bias: 7.805428, T: 4658640, Avg. loss: 0.019248\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 1.75, NNZs: 11, Bias: 7.805966, T: 4697462, Avg. loss: 0.019250\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 1.75, NNZs: 11, Bias: 7.804857, T: 4736284, Avg. loss: 0.019252\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 1.75, NNZs: 11, Bias: 7.808462, T: 4775106, Avg. loss: 0.019251\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 1.75, NNZs: 11, Bias: 7.807275, T: 4813928, Avg. loss: 0.019247\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 1.75, NNZs: 11, Bias: 7.801731, T: 4852750, Avg. loss: 0.019245\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 1.75, NNZs: 11, Bias: 7.802066, T: 4891572, Avg. loss: 0.019244\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 1.76, NNZs: 11, Bias: 7.805528, T: 4930394, Avg. loss: 0.019249\n",
      "Total training time: 1.29 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 1.75, NNZs: 11, Bias: 7.808235, T: 4969216, Avg. loss: 0.019247\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 1.76, NNZs: 11, Bias: 7.806878, T: 5008038, Avg. loss: 0.019248\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 1.76, NNZs: 11, Bias: 7.804726, T: 5046860, Avg. loss: 0.019247\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 1.76, NNZs: 11, Bias: 7.807174, T: 5085682, Avg. loss: 0.019240\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 1.76, NNZs: 11, Bias: 7.806497, T: 5124504, Avg. loss: 0.019240\n",
      "Total training time: 1.34 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 1.76, NNZs: 11, Bias: 7.806175, T: 5163326, Avg. loss: 0.019247\n",
      "Total training time: 1.35 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 1.76, NNZs: 11, Bias: 7.803270, T: 5202148, Avg. loss: 0.019239\n",
      "Total training time: 1.36 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 1.76, NNZs: 11, Bias: 7.806598, T: 5240970, Avg. loss: 0.019232\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 1.76, NNZs: 11, Bias: 7.806289, T: 5279792, Avg. loss: 0.019236\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 1.76, NNZs: 11, Bias: 7.804507, T: 5318614, Avg. loss: 0.019233\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 1.76, NNZs: 11, Bias: 7.804716, T: 5357436, Avg. loss: 0.019235\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 1.76, NNZs: 11, Bias: 7.806799, T: 5396258, Avg. loss: 0.019235\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 1.76, NNZs: 11, Bias: 7.804787, T: 5435080, Avg. loss: 0.019237\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 1.76, NNZs: 11, Bias: 7.805482, T: 5473902, Avg. loss: 0.019235\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 1.76, NNZs: 11, Bias: 7.808338, T: 5512724, Avg. loss: 0.019236\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 1.77, NNZs: 11, Bias: 7.804792, T: 5551546, Avg. loss: 0.019235\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 1.77, NNZs: 11, Bias: 7.804861, T: 5590368, Avg. loss: 0.019233\n",
      "Total training time: 1.47 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 1.77, NNZs: 11, Bias: 7.805736, T: 5629190, Avg. loss: 0.019226\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 1.77, NNZs: 11, Bias: 7.804956, T: 5668012, Avg. loss: 0.019237\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 1.77, NNZs: 11, Bias: 7.806971, T: 5706834, Avg. loss: 0.019232\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 1.77, NNZs: 11, Bias: 7.809668, T: 5745656, Avg. loss: 0.019230\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 1.77, NNZs: 11, Bias: 7.805009, T: 5784478, Avg. loss: 0.019228\n",
      "Total training time: 1.52 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 1.77, NNZs: 11, Bias: 7.808017, T: 5823300, Avg. loss: 0.019229\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 1.77, NNZs: 11, Bias: 7.805488, T: 5862122, Avg. loss: 0.019229\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 1.77, NNZs: 11, Bias: 7.807088, T: 5900944, Avg. loss: 0.019230\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 1.77, NNZs: 11, Bias: 7.804106, T: 5939766, Avg. loss: 0.019222\n",
      "Total training time: 1.57 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 1.77, NNZs: 11, Bias: 7.804232, T: 5978588, Avg. loss: 0.019228\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 1.78, NNZs: 11, Bias: 7.803607, T: 6017410, Avg. loss: 0.019226\n",
      "Total training time: 1.59 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 1.78, NNZs: 11, Bias: 7.805889, T: 6056232, Avg. loss: 0.019223\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 1.78, NNZs: 11, Bias: 7.807585, T: 6095054, Avg. loss: 0.019225\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 1.78, NNZs: 11, Bias: 7.805421, T: 6133876, Avg. loss: 0.019229\n",
      "Total training time: 1.62 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 1.78, NNZs: 11, Bias: 7.807112, T: 6172698, Avg. loss: 0.019217\n",
      "Total training time: 1.63 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 1.78, NNZs: 11, Bias: 7.806033, T: 6211520, Avg. loss: 0.019219\n",
      "Total training time: 1.64 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 1.78, NNZs: 11, Bias: 7.806562, T: 6250342, Avg. loss: 0.019226\n",
      "Total training time: 1.65 seconds.\n",
      "-- Epoch 162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 1.78, NNZs: 11, Bias: 7.804121, T: 6289164, Avg. loss: 0.019222\n",
      "Total training time: 1.66 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 1.78, NNZs: 11, Bias: 7.807617, T: 6327986, Avg. loss: 0.019225\n",
      "Total training time: 1.67 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 1.78, NNZs: 11, Bias: 7.808078, T: 6366808, Avg. loss: 0.019225\n",
      "Total training time: 1.68 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 1.78, NNZs: 11, Bias: 7.804983, T: 6405630, Avg. loss: 0.019219\n",
      "Total training time: 1.69 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 1.78, NNZs: 11, Bias: 7.804059, T: 6444452, Avg. loss: 0.019220\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 1.78, NNZs: 11, Bias: 7.806920, T: 6483274, Avg. loss: 0.019217\n",
      "Total training time: 1.71 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 1.78, NNZs: 11, Bias: 7.807377, T: 6522096, Avg. loss: 0.019216\n",
      "Total training time: 1.72 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 1.78, NNZs: 11, Bias: 7.806060, T: 6560918, Avg. loss: 0.019215\n",
      "Total training time: 1.73 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 1.78, NNZs: 11, Bias: 7.803843, T: 6599740, Avg. loss: 0.019212\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 1.79, NNZs: 11, Bias: 7.805514, T: 6638562, Avg. loss: 0.019217\n",
      "Total training time: 1.75 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 1.79, NNZs: 11, Bias: 7.806919, T: 6677384, Avg. loss: 0.019215\n",
      "Total training time: 1.76 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 1.79, NNZs: 11, Bias: 7.809216, T: 6716206, Avg. loss: 0.019215\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 1.79, NNZs: 11, Bias: 7.803564, T: 6755028, Avg. loss: 0.019211\n",
      "Total training time: 1.78 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 1.79, NNZs: 11, Bias: 7.806573, T: 6793850, Avg. loss: 0.019215\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 1.79, NNZs: 11, Bias: 7.805404, T: 6832672, Avg. loss: 0.019214\n",
      "Total training time: 1.80 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 1.79, NNZs: 11, Bias: 7.803693, T: 6871494, Avg. loss: 0.019214\n",
      "Total training time: 1.81 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 1.79, NNZs: 11, Bias: 7.806489, T: 6910316, Avg. loss: 0.019218\n",
      "Total training time: 1.82 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 1.79, NNZs: 11, Bias: 7.808421, T: 6949138, Avg. loss: 0.019212\n",
      "Total training time: 1.83 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 1.79, NNZs: 11, Bias: 7.805866, T: 6987960, Avg. loss: 0.019213\n",
      "Total training time: 1.84 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 1.79, NNZs: 11, Bias: 7.807178, T: 7026782, Avg. loss: 0.019211\n",
      "Total training time: 1.85 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 1.79, NNZs: 11, Bias: 7.808190, T: 7065604, Avg. loss: 0.019215\n",
      "Total training time: 1.87 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 1.80, NNZs: 11, Bias: 7.803541, T: 7104426, Avg. loss: 0.019206\n",
      "Total training time: 1.88 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 1.79, NNZs: 11, Bias: 7.804668, T: 7143248, Avg. loss: 0.019210\n",
      "Total training time: 1.89 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 1.80, NNZs: 11, Bias: 7.807010, T: 7182070, Avg. loss: 0.019211\n",
      "Total training time: 1.90 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 1.80, NNZs: 11, Bias: 7.805642, T: 7220892, Avg. loss: 0.019214\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 1.80, NNZs: 11, Bias: 7.803256, T: 7259714, Avg. loss: 0.019200\n",
      "Total training time: 1.92 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 1.80, NNZs: 11, Bias: 7.804754, T: 7298536, Avg. loss: 0.019202\n",
      "Total training time: 1.93 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 1.80, NNZs: 11, Bias: 7.805818, T: 7337358, Avg. loss: 0.019202\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 1.80, NNZs: 11, Bias: 7.804664, T: 7376180, Avg. loss: 0.019206\n",
      "Total training time: 1.95 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 1.80, NNZs: 11, Bias: 7.804182, T: 7415002, Avg. loss: 0.019204\n",
      "Total training time: 1.96 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 1.80, NNZs: 11, Bias: 7.806115, T: 7453824, Avg. loss: 0.019206\n",
      "Total training time: 1.97 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 1.80, NNZs: 11, Bias: 7.809097, T: 7492646, Avg. loss: 0.019202\n",
      "Total training time: 1.98 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 1.80, NNZs: 11, Bias: 7.804923, T: 7531468, Avg. loss: 0.019200\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 1.80, NNZs: 11, Bias: 7.806867, T: 7570290, Avg. loss: 0.019210\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 1.80, NNZs: 11, Bias: 7.804359, T: 7609112, Avg. loss: 0.019203\n",
      "Total training time: 2.02 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 1.81, NNZs: 11, Bias: 7.805475, T: 7647934, Avg. loss: 0.019200\n",
      "Total training time: 2.03 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 1.81, NNZs: 11, Bias: 7.805957, T: 7686756, Avg. loss: 0.019203\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 1.81, NNZs: 11, Bias: 7.806327, T: 7725578, Avg. loss: 0.019202\n",
      "Total training time: 2.05 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 1.81, NNZs: 11, Bias: 7.805178, T: 7764400, Avg. loss: 0.019200\n",
      "Total training time: 2.06 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 1.81, NNZs: 11, Bias: 7.806030, T: 7803222, Avg. loss: 0.019199\n",
      "Total training time: 2.07 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 1.81, NNZs: 11, Bias: 7.804991, T: 7842044, Avg. loss: 0.019204\n",
      "Total training time: 2.08 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 1.81, NNZs: 11, Bias: 7.802081, T: 7880866, Avg. loss: 0.019193\n",
      "Total training time: 2.09 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 1.81, NNZs: 11, Bias: 7.802947, T: 7919688, Avg. loss: 0.019195\n",
      "Total training time: 2.10 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 1.81, NNZs: 11, Bias: 7.805833, T: 7958510, Avg. loss: 0.019200\n",
      "Total training time: 2.11 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 1.81, NNZs: 11, Bias: 7.803714, T: 7997332, Avg. loss: 0.019199\n",
      "Total training time: 2.12 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 1.81, NNZs: 11, Bias: 7.806327, T: 8036154, Avg. loss: 0.019188\n",
      "Total training time: 2.13 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 1.81, NNZs: 11, Bias: 7.808314, T: 8074976, Avg. loss: 0.019192\n",
      "Total training time: 2.14 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 1.81, NNZs: 11, Bias: 7.806433, T: 8113798, Avg. loss: 0.019199\n",
      "Total training time: 2.15 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 1.81, NNZs: 11, Bias: 7.806654, T: 8152620, Avg. loss: 0.019194\n",
      "Total training time: 2.16 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 1.81, NNZs: 11, Bias: 7.806169, T: 8191442, Avg. loss: 0.019194\n",
      "Total training time: 2.18 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 1.82, NNZs: 11, Bias: 7.806790, T: 8230264, Avg. loss: 0.019197\n",
      "Total training time: 2.19 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 1.82, NNZs: 11, Bias: 7.805499, T: 8269086, Avg. loss: 0.019197\n",
      "Total training time: 2.20 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 1.82, NNZs: 11, Bias: 7.802818, T: 8307908, Avg. loss: 0.019195\n",
      "Total training time: 2.21 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 1.82, NNZs: 11, Bias: 7.806758, T: 8346730, Avg. loss: 0.019191\n",
      "Total training time: 2.22 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 1.82, NNZs: 11, Bias: 7.806376, T: 8385552, Avg. loss: 0.019198\n",
      "Total training time: 2.23 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 1.82, NNZs: 11, Bias: 7.803820, T: 8424374, Avg. loss: 0.019194\n",
      "Total training time: 2.24 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 1.82, NNZs: 11, Bias: 7.803057, T: 8463196, Avg. loss: 0.019189\n",
      "Total training time: 2.25 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 1.82, NNZs: 11, Bias: 7.807669, T: 8502018, Avg. loss: 0.019193\n",
      "Total training time: 2.25 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 1.82, NNZs: 11, Bias: 7.807384, T: 8540840, Avg. loss: 0.019196\n",
      "Total training time: 2.26 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 1.82, NNZs: 11, Bias: 7.804441, T: 8579662, Avg. loss: 0.019194\n",
      "Total training time: 2.27 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 1.82, NNZs: 11, Bias: 7.803550, T: 8618484, Avg. loss: 0.019190\n",
      "Total training time: 2.28 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 1.82, NNZs: 11, Bias: 7.804265, T: 8657306, Avg. loss: 0.019191\n",
      "Total training time: 2.29 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 1.82, NNZs: 11, Bias: 7.807207, T: 8696128, Avg. loss: 0.019187\n",
      "Total training time: 2.30 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 1.82, NNZs: 11, Bias: 7.805179, T: 8734950, Avg. loss: 0.019191\n",
      "Total training time: 2.31 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 1.83, NNZs: 11, Bias: 7.807870, T: 8773772, Avg. loss: 0.019187\n",
      "Total training time: 2.32 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 1.83, NNZs: 11, Bias: 7.807695, T: 8812594, Avg. loss: 0.019186\n",
      "Total training time: 2.33 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 1.83, NNZs: 11, Bias: 7.804883, T: 8851416, Avg. loss: 0.019191\n",
      "Total training time: 2.34 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 1.83, NNZs: 11, Bias: 7.805742, T: 8890238, Avg. loss: 0.019183\n",
      "Total training time: 2.35 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 1.83, NNZs: 11, Bias: 7.803404, T: 8929060, Avg. loss: 0.019186\n",
      "Total training time: 2.36 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 1.83, NNZs: 11, Bias: 7.805072, T: 8967882, Avg. loss: 0.019183\n",
      "Total training time: 2.37 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 1.83, NNZs: 11, Bias: 7.806239, T: 9006704, Avg. loss: 0.019186\n",
      "Total training time: 2.38 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 1.83, NNZs: 11, Bias: 7.803778, T: 9045526, Avg. loss: 0.019188\n",
      "Total training time: 2.39 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 1.83, NNZs: 11, Bias: 7.802513, T: 9084348, Avg. loss: 0.019182\n",
      "Total training time: 2.40 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 1.83, NNZs: 11, Bias: 7.802064, T: 9123170, Avg. loss: 0.019190\n",
      "Total training time: 2.41 seconds.\n",
      "-- Epoch 236\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0f70d0f10a3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mbest_hyperparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'invscaling'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_iter'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eta0'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mregr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklLinearRegressionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_hyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-0f70d0f10a3a>\u001b[0m in \u001b[0;36msklLinearRegressionModel\u001b[0;34m(X_train, y_train, best_hyperparams)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msklLinearRegressionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_hyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mregression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGDRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbest_hyperparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mregression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m   1053\u001b[0m                          \u001b[0mcoef_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m                          \u001b[0mintercept_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m                          sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         self._partial_fit(X, y, alpha, C, loss, learning_rate,\n\u001b[1;32m   1014\u001b[0m                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m                           intercept_init)\n\u001b[0m\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m         if (self._tol is not None and self._tol > -np.inf\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         self._fit_regressor(X, y, alpha, C, loss, learning_rate,\n\u001b[0;32m--> 962\u001b[0;31m                             sample_weight, max_iter)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit_regressor\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, sample_weight, max_iter)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                           \u001b[0mlearning_rate_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                           intercept_decay)\n\u001b[0m\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/linear_model/sgd_fast.pyx\u001b[0m in \u001b[0;36msklearn.linear_model.sgd_fast.plain_sgd\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/linear_model/sgd_fast.pyx\u001b[0m in \u001b[0;36msklearn.linear_model.sgd_fast._plain_sgd\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "def sklLinearRegressionModel(X_train,y_train,best_hyperparams):\n",
    "\tregression = linear_model.SGDRegressor(**best_hyperparams, penalty=None, verbose=True) \n",
    "\tregression.fit(X_train, np.log(y_train))\n",
    "\treturn regression\n",
    "\n",
    "\n",
    "def crossValidationKFold(X_train,y_train):\n",
    "\thyperparams = {\n",
    "    \t'learning_rate':['invscaling', 'optimal'],\n",
    "    \t'eta0': [0.1, 0.05, 0.01], \n",
    "    \t'max_iter':[20000, 10000]\n",
    "\t}\n",
    "\n",
    "\tscoring = {\n",
    "    \t'NEG_MSE': 'neg_mean_squared_error',\n",
    "    \t'NEG_MAE': 'neg_mean_absolute_error',\n",
    "    \t'VARIANCE': 'r2'\n",
    "\t}\n",
    "\n",
    "\tregr = GridSearchCV(linear_model.SGDRegressor(), hyperparams, cv=5, scoring=scoring, refit='VARIANCE', n_jobs=-1, verbose=True)\n",
    "\n",
    "\tregr.fit(X_train, np.log(y_train))\n",
    "\n",
    "\treturn regr.best_params_\n",
    "\n",
    "\n",
    "best_hyperparams = {'learning_rate': 'invscaling', 'max_iter': 2000, 'eta0': 0.01}\n",
    "regr = sklLinearRegressionModel(X_train,y_train,best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "49d3838ff0763e3ef739ce6476d5acf274626b8d"
   },
   "outputs": [],
   "source": [
    "scoring = {\n",
    "    \t'NEG_MSE': 'neg_mean_squared_error',\n",
    "    \t'NEG_MAE': 'neg_mean_absolute_error',\n",
    "    \t'VARIANCE': 'r2'\n",
    "\t}\n",
    "\n",
    "for k, score in scoring.items(): \n",
    "    regr.verbose = False\n",
    "    scores = model_selection.cross_val_score(regr, X_train, y_train, cv=5, scoring=score, n_jobs=-1, verbose=True)\n",
    "    print (k,  np.mean(scores), np.std(scores))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b8fcfe10618eddce897beb586ab26b66c9f4a5bf"
   },
   "outputs": [],
   "source": [
    "# regr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "839e38e291f09bf8ea0bf46b1403121ac95804f9"
   },
   "outputs": [],
   "source": [
    "y_val.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d3ed27c572307c9ebf1a6172efef9e09ccfd4f6"
   },
   "outputs": [],
   "source": [
    "y_pred = np.exp(regr.predict(X_val))\n",
    "pd.Series(y_pred).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0522010985023ed087796f90d5db4d0a43f3912b"
   },
   "outputs": [],
   "source": [
    "print(\"MSE: %.3f\" % metrics.mean_squared_error(y_val, y_pred))\n",
    "print(\"MAE: %.3f\" % metrics.mean_absolute_error(y_val, y_pred))\n",
    "print('R2: %.3f' % metrics.r2_score(y_val, y_pred))\n",
    "\n",
    "plt.hist(y_val, bins=100, color='blue', linewidth=3)\n",
    "plt.show()\n",
    "plt.hist(y_pred, bins=100, color='red', linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "36fd43759e56554db0c8e3f1b53df6aeea7d4c95"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'real': y_val, 'pred': y_pred})\n",
    "ax = df.sort_values('real').plot.scatter('real', 'pred', figsize=(5, 5))\n",
    "_ = ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9dfc937025b0da986f7a5ad9dfe634d437ca00ea"
   },
   "outputs": [],
   "source": [
    "def oneHotEncoding(features,columnName):\n",
    "\tcurrentCol = features.columns.get_loc(columnName)\n",
    "\tuniqueFeatures = features[columnName].unique()\n",
    "\tprint(uniqueFeatures)\n",
    "\tfor f in range(len(uniqueFeatures)):\n",
    "\t\tfeatures.insert(loc=currentCol+f,column=columnName+str(f),value=0)\n",
    "\t\tfeatures[columnName+str(f)][features[columnName]==uniqueFeatures[f]] = 1\n",
    "\t\t\n",
    "\tfeatures.pop(columnName)\n",
    "\n",
    "def dummieCoding(features,columnName,orderedFeature):\n",
    "\tc = 0\n",
    "\tfor f in range(len(orderedFeature)):\n",
    "\t\tfeatures[columnName][features[columnName]==orderedFeature[f]] = 2**c\n",
    "\t\tc = c + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "486ea6525308cc1ad2dc56654aad527014d2e860"
   },
   "outputs": [],
   "source": [
    "def RMSE(theta,X,y):\n",
    "\ty_linha = hypothesis(theta,X)\n",
    "\terror = math.sqrt(((y_linha-y)**2).mean())\n",
    "\treturn error\n",
    "\n",
    "#\n",
    "#  For linear regression, it is possible to estimate the values\n",
    "# of all parameters theta by applying the normal equation method,\n",
    "# which corresponds to the following equation:\n",
    "#\n",
    "#  Theta = (Xt.X)^-1.Xt.y\n",
    "#\n",
    "#  This procedure is called Normal Equation, which is implemented\n",
    "# here\n",
    "#\n",
    "# params:\n",
    "#   X -> set of features\n",
    "#   Y -> set of targets\n",
    "#\n",
    "# return:\n",
    "#   theta -> set of parameters\n",
    "#\n",
    "def normalEquation(X,y):\n",
    "\tX = np.insert(X,0,1,axis=1)\n",
    "\tnpX = np.copy(X)\n",
    "\tnpY = y.transpose()\n",
    "\tnpXt = npX.transpose()\n",
    "\n",
    "\tR1 = np.matmul(npXt,npX)\n",
    "\n",
    "\tdet = np.linalg.det(R1)\n",
    "\n",
    "\tif (det != 0):\n",
    "\t\tR1 = np.linalg.inv(R1)\n",
    "\t\tR2 = np.matmul(npXt,npY)\n",
    "\t\ttheta = np.matmul(R1,R2)\n",
    "\telse:\n",
    "\t\ttheta = []\n",
    "\t\tprint(\"Error! Matrix (Xt.X) has no inverse.\")\n",
    "\n",
    "\terror = RMSE(theta,X,y)\n",
    "\tprint(\"Normal Equation --- RMSE error: \",str(error))\n",
    "\n",
    "\tX = np.delete(X,0,axis=1)\n",
    "\n",
    "\treturn theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e46a79d263d96f3c889255b1415fb1c8ba8f1467"
   },
   "outputs": [],
   "source": [
    "theta = np.array([1, 0, 1], dtype=np.double)\n",
    "theta_temp = np.array([0, 0, 0], dtype=np.double)\n",
    "y = np.array([5.,10.], dtype=np.double)\n",
    "X = np.array([[0.,1., 2.],[0.,2., 3.]], dtype=np.double)\n",
    "print (X)\n",
    "alpha = .01\n",
    "max_iter = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "97e1db86446eae07a972b9eb0d81fa75aed544dd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hypothesis(theta, X):\n",
    "    return np.sum(theta.T * X, axis=1)\n",
    "    \n",
    "def MSE_theta(theta, X, y, alpha,j, h0, error):                \n",
    "        S = np.sum(np.matmul(error, X[:,j]))                \n",
    "        result = theta[j] - (alpha * (1. / len(y)) * S)        \n",
    "        return result\n",
    "\n",
    "for i in range(max_iter):\n",
    "    h0 = hypothesis(theta, X)\n",
    "    error = (h0 - y)\n",
    "    for j in range(X.shape[1]):\n",
    "        theta_temp[j] = MSE_theta(theta, X, y, alpha, j, h0, error)    \n",
    "        \n",
    "    theta = theta_temp.copy()\n",
    "    print (theta)    \n",
    "\n",
    "hypothesis(theta, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d9e0c1753391fe050a75359d8be53ad9684bcdf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "def SGD_(alpha, max_iter, X, y):\n",
    "    \n",
    "    # Creating theta0 \n",
    "    X = np.insert(X, values=1, obj=0, axis=1)\n",
    "    \n",
    "    shape = X.shape\n",
    "    nsamples = shape[0]\n",
    "    print(\"Number of samples: \"+str(nsamples))\n",
    "    theta0 = np.zeros(nsamples)\n",
    "    nparams = shape[1]\n",
    "    print(\"Number of parameters: \"+str(nparams))\n",
    "\n",
    "\n",
    "    theta = np.random.uniform(size=nparams)\n",
    "    theta_temp = np.ones(nparams)\n",
    "\n",
    "    error = 1\n",
    "    epsilon = 0.001\n",
    "    it = 0\n",
    "    i = 0   \n",
    "    power_t = 0.25\n",
    "    t=1.0\n",
    "    \n",
    "    while ((error > epsilon) and (it < max_iter)):\n",
    "        h0 = hypothesis(theta, X)\n",
    "        eta = alpha / pow(t, power_t)\n",
    "        error = (h0 - y)\n",
    "        for j in range(nparams):\n",
    "            theta_temp[j] = MSE_theta(theta, X, y, eta, j, h0, error)                \n",
    "        it += 1\n",
    "        i += 1\n",
    "        y_pred = hypothesis(theta_temp, X)\n",
    "#         print (y,hyphotesis(theta_temp, X))\n",
    "        error =  ((y - y_pred) ** 2).mean() / 2 \n",
    "#         print(theta)\n",
    "#         print(theta_temp)\n",
    "\n",
    "        theta = theta_temp.copy()\n",
    "        \n",
    "        if (i % 100) == 0 or i == 1:\n",
    "            print(\"Epoch: %s Batch: %s Error: %.8f lr: %.8f \"%(it, i, error, eta))\n",
    "        t += 1            \n",
    "   \n",
    "    return theta\n",
    "def predict(theta, X):\n",
    "    X = np.insert(X, values=1, obj=0, axis=1)\n",
    "    return hypothesis(theta_h, X)\n",
    "\n",
    "max_iter = 10000\n",
    "theta_h = SGD_(alpha, max_iter, X, y)\n",
    "print (y,predict(theta_h, X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f12b2d1321c703975d08f97db37c9cb3677c458a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_iter = 10000\n",
    "theta_h = SGD_(1., max_iter=max_iter, X=X_train, y=np.log(y_train.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "36dc9621a15f9bf1aab6eb11600f31b5e379f252"
   },
   "outputs": [],
   "source": [
    "y_pred = np.exp(predict(theta_h, X_val))\n",
    "\n",
    "df = pd.DataFrame({'real': y_val, 'pred': y_pred})\n",
    "ax = df.sort_values('real').plot.scatter('real', 'pred', figsize=(5, 5))\n",
    "_ = ax.plot([y_val.min(), y_val.max()], [y_pred.min(), y_pred.max()], 'k--', lw=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "95b777cdd28ea30d7ce6b722db63208000e0f00a"
   },
   "outputs": [],
   "source": [
    "np.mean((np.log(y_pred) - np.log(y_val.values))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8445b674ec0ce22a184fa0f05f9bdde285c86775"
   },
   "outputs": [],
   "source": [
    "np.mean((y_pred - y_val.values)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b0979fdde25217dcf481a705d5ca2bf968ee67f"
   },
   "outputs": [],
   "source": [
    "print(\"MSE: %.3f\" % metrics.mean_squared_error(y_val, y_pred))\n",
    "print(\"MAE: %.3f\" % metrics.mean_absolute_error(y_val, y_pred))\n",
    "print('R2: %.3f' % metrics.r2_score(y_val, y_pred))\n",
    "\n",
    "plt.hist(y_val, bins=100, color='blue', linewidth=3)\n",
    "plt.show()\n",
    "plt.hist(y_pred, bins=100, color='red', linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_h = normalEquation(X_train, y=np.log(y_train.values))\n",
    "y_pred = np.exp(predict(theta_h, X_val))\n",
    "print(\"MSE: %.3f\" % metrics.mean_squared_error(y_val, y_pred))\n",
    "print(\"MAE: %.3f\" % metrics.mean_absolute_error(y_val, y_pred))\n",
    "print('R2: %.3f' % metrics.r2_score(y_val, y_pred))\n",
    "\n",
    "plt.hist(y_val, bins=100, color='blue', linewidth=3)\n",
    "plt.show()\n",
    "plt.hist(y_pred, bins=100, color='red', linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_h = normalEquation(X_train, y=y_train.values)\n",
    "y_pred = predict(theta_h, X_val)\n",
    "print(\"MSE: %.3f\" % metrics.mean_squared_error(y_val, y_pred))\n",
    "print(\"MAE: %.3f\" % metrics.mean_absolute_error(y_val, y_pred))\n",
    "print('R2: %.3f' % metrics.r2_score(y_val, y_pred))\n",
    "\n",
    "plt.hist(y_val, bins=100, color='blue', linewidth=3)\n",
    "plt.show()\n",
    "plt.hist(y_pred, bins=100, color='red', linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
